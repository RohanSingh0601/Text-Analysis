# -*- coding: utf-8 -*-
"""DataCollection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hatx5I3a-jJlPIcgERuKSeB7JoUBrQfH
"""

import numpy as np
import pandas as pd
import re
# make http requests and webscrap
import requests

# Plotting Libraries
import seaborn as sb
from matplotlib import pyplot as plt

# text processing
import string
import nltk
from nltk.corpus import stopwords

#!pip install arxiv bs4

from bs4 import BeautifulSoup
import arxiv

def get_snarXiv_abstracts(n_queries = 1):
    # make a GET request to the snarXiv page
    both = []
    abstracts = []
    titles = []
    for n in range(n_queries):
        with requests.get('http://snarxiv.org/') as response:
            # Let's scrap the html document of the page
            soup = BeautifulSoup(response.content)
            # The abstracts are inside <p> tags which are
            # inside of <div class = "meta"> tags.
            abstracts.extend( map(lambda x:x.text,soup.select("dd > div.meta > p")) )
            titles.extend( map(lambda x:x.text,soup.select("dd > div.meta > div.list-title")) )
    both.append(abstracts)
    both.append(titles)
    return both

# Function to create a dataframe with the snarXiv abstracts
def make_snarXiv_dataframe(n_queries = 1):
    summaries = get_snarXiv_abstracts(n_queries = n_queries)
    abstracts = summaries[0]
    titles = summaries[1]
    summaries_length = len(abstracts)
    labels = ['snarXiv']*summaries_length
    return pd.DataFrame(
        data = {
            'summary':abstracts,
            'title':titles,
            'label':labels
        }
    ).rename_axis('id')



#make_snarXiv_dataframe(n_queries = 40)

snar_data = make_snarXiv_dataframe(100)

snar_data['title'] = snar_data['title'].str[8:]



#snar_data.to_csv('snarxiv_data.csv', index = False)

snar_data['title'] = snar_data['title'].str.rstrip('\n')

snar_data

snar_data.to_csv('snarxiv_data.csv', index = False)

test = pd.read_csv('snarxiv_data.csv')

test

test.head(20)

#import json

#df = pd.read_json('arxiv-metadata-oai-snapshot.json', lines = True)

def get_arXiv_preprints(n_abstracts = 10):
    results = arxiv.Search(
        query = 'cat:hep-th',
        sort_by = 'submittedDate',
        sort_order = 'descending',
        max_results = n_abstracts
    )
    # Some of the results aren't actually in hep-th, so I will remove them
    return [result for result in results if result['arxiv_primary_category']['term']=='hep-th']

# Function to create a dataframe with the arXiv abstracts
def make_arXiv_dataframe(n_abstracts = 10):
    results = get_arXiv_preprints(n_abstracts = n_abstracts)
    arxiv_columns = [
        'id',
        'summary'
    ]
    arxiv_data = pd.DataFrame({key:result[key] for key in arxiv_columns} for result in results).set_index('id')
    arxiv_data['label'] = 'arXiv'
    return arxiv_data

make_arXiv_dataframe()

search = arxiv.Search(
  query = 'cat:hep-th',
  max_results = 1000,
  sort_by = arxiv.SortCriterion.SubmittedDate
)

for result in search.results():
    print(result.title)

results_generator = arxiv.Client(
  page_size=1000,
  delay_seconds=3,
  num_retries=3
).results(arxiv.Search(
  query='hep-th',
  max_results = 250,
  id_list=[],
  sort_by=arxiv.SortCriterion.Relevance,
  sort_order=arxiv.SortOrder.Descending,
))

quantum_dots = []
for paper in results_generator:
  # You could do per-paper analysis here; for now, just collect them in a list.
  quantum_dots.append(paper)

qd_df = pd.DataFrame([vars(paper) for paper in quantum_dots])

qd_df

results_generator = arxiv.Client(
  page_size=1000,
  delay_seconds=3,
  num_retries=3
).results(arxiv.Search(
  query='hep-ph',
  max_results = 250,
  id_list=[],
  sort_by=arxiv.SortCriterion.Relevance,
  sort_order=arxiv.SortOrder.Descending,
))

ph = []
for paper in results_generator:
  # You could do per-paper analysis here; for now, just collect them in a list.
  ph.append(paper)

ph_df = pd.DataFrame([vars(paper) for paper in ph])

ph_df

th_df = qd_df[['title','summary']]

th_df['label'] = 'arXiv'

th_df

ph_df = ph_df[['title', 'summary']]
ph_df['label'] = 'arXiv'
ph_df

arxiv_df = pd.concat([th_df, ph_df])

arxiv_df

df = pd.concat([test, arxiv_df])

df

df.to_csv('dataset1000.csv', index = False)

df = pd.read_csv('dataset1000.csv')

df

# pre-processing
# check out nfx
import neattext.functions as nfx

dir(nfx)

df['summary'] = df['summary'].apply(nfx.remove_stopwords)

df['title'] = df['title'].apply(nfx.remove_stopwords)

df

df['summary'] = df['summary'].apply(nfx.remove_punctuations)
df['title'] = df['title'].apply(nfx.remove_punctuations)

df

df.to_csv('df1000_cleaned.csv', index = False)

########
########
# lemmatize
#

import nltk
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

df = pd.read_csv('df1000_cleaned.csv')

df

print(df.isnull().values.any())
print(df.isnull().sum())

df = df.dropna()

df.to_csv('df1000_cleaned.csv', index = False)

df

df = df.apply(lambda x: x.astype(str).str.lower())

df











lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    words = text.split()
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
    return ' '.join(lemmatized_words)

df['summary'] = df['summary'].map(lemmatize_text)

df['title'] = df['title'].map(lemmatize_text)





df

df.to_csv('df1000_cleaned.csv', index = False)

df





df['title']

df.dtypes





df = pd.read_csv('df1000_cleaned.csv')

df

df['title'] = df['title'].str.replace(r'\(|\)', '')

df = df.drop_duplicates(subset = ['title']).reset_index()
df = df[['title', 'label']]

test_snarxiv = df.iloc[:100]
test_arxiv = df.iloc[-100:]
training_df = df.iloc[100:-100]

training_df

training_df.to_csv('training_data.csv', index = False)

testing_df = pd.concat([test_snarxiv, test_arxiv], axis = 0)

testing_df

testing_df.to_csv('testing_data.csv', index = False)

